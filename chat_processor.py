import os
import pandas as pd
import re
from typing import Dict, Any, List
import logging
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
import math
import json
import asyncio
import re
from astrapy import DataAPIClient
from openai import OpenAI
from memory import ConversationMemory
from langchain_anthropic import ChatAnthropic

# Config imports
from config import (
    LLMConfigs, LLMProvider, VectorConfig, CSVConfig,
    PromptTemplates, CSV_KEYWORDS,
    DatabaseSettings, MessageSettings
)

logger = logging.getLogger(__name__)

class LLMFactory:
    @staticmethod
    def create_llm(config):
        """GeliÅŸmiÅŸ hata yÃ¶netimi ile LLM oluÅŸtur"""
        try:
            params = config.to_langchain_params()
            logger.info(f"ğŸ¤– LLM oluÅŸturuluyor: {config.provider.value} - {config.model}")
            
            if config.provider == LLMProvider.OPENAI:
                from langchain_openai import ChatOpenAI
                return ChatOpenAI(**params)
            elif config.provider == LLMProvider.GOOGLE:
                from langchain_google_genai import ChatGoogleGenerativeAI
                return ChatGoogleGenerativeAI(**params)
            elif config.provider == LLMProvider.ANTHROPIC:
                from langchain_anthropic import ChatAnthropic
                return ChatAnthropic(**params)
            else:
                raise ValueError(f"Desteklenmeyen provider: {config.provider}")
                
        except Exception as e:
            logger.error(f"âŒ LLM oluÅŸturma hatasÄ± ({config.provider.value}): {e}")
            raise

class TercihAsistaniProcessor:
    """
    HIZLANDIRILMIÅ VERSION - Railway timeout fix
    """
    
    def __init__(self):
        self.llm_evaluation = None
        self.llm_correction = None
        self.llm_search_optimizer = None
        self.llm_csv_agent = None
        self.llm_final = None
        
        # Native components
        self.openai_client = None
        self.astra_database = None
        self.astra_collection = None
        
        self.csv_data = None
        self.memory = ConversationMemory() 
        
        # Config'lerden prompt'larÄ± al
        self.evaluation_prompt = ChatPromptTemplate.from_template(PromptTemplates.EVALUATION)
        self.correction_prompt = ChatPromptTemplate.from_template(PromptTemplates.CORRECTION)
        self.search_optimizer_prompt = ChatPromptTemplate.from_template(PromptTemplates.SEARCH_OPTIMIZER)
        self.csv_agent_prompt = ChatPromptTemplate.from_template(PromptTemplates.CSV_AGENT)
        self.final_prompt = ChatPromptTemplate.from_template(PromptTemplates.FINAL_RESPONSE)

    def get_embedding(self, text: str) -> List[float]:
        """OpenAI embedding oluÅŸtur"""
        try:
            response = self.openai_client.embeddings.create(
                input=text,
                model="text-embedding-3-small"
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"âŒ Embedding oluÅŸturma hatasÄ±: {e}")
            raise

    async def initialize(self):
        """HIZLANDIRILMIÅ baÅŸlatma"""
        try:
            logger.info("ğŸš€ TercihAsistaniProcessor baÅŸlatÄ±lÄ±yor...")
            
            # API Key kontrolÃ¼ (basit)
            if not os.getenv("OPENAI_API_KEY"):
                raise ValueError("OPENAI_API_KEY zorunlu!")
            
            # OpenAI client'Ä± baÅŸlat
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            logger.info("âœ… OpenAI client baÅŸlatÄ±ldÄ±")
            
            # HIZLI LLM'leri paralel baÅŸlat
            await self._initialize_llms_fast()
            
            # AstraDB baÄŸlantÄ±sÄ±
            await self._initialize_astradb_native()
            
            # CSV verilerini yÃ¼kle
            await self._initialize_csv()
                
            logger.info("âœ… TercihAsistaniProcessor baÅŸarÄ±yla baÅŸlatÄ±ldÄ±")
            
        except Exception as e:
            logger.error(f"âŒ Initialization hatasÄ±: {e}")
            raise

    async def _initialize_llms_fast(self):
        """SADECE OPENAI modelleri - maksimum hÄ±z"""
        try:
            # TÃ¼m kritik iÅŸlemler iÃ§in hÄ±zlÄ± OpenAI modelleri
            fast_config = LLMConfig(
                provider=LLMProvider.OPENAI,
                model="gpt-4o-mini",
                temperature=0.3,
                max_tokens=300,
                timeout=15  # 15 saniye timeout
            )
            
            # TÃ¼m LLM'ler aynÄ± hÄ±zlÄ± model
            self.llm_evaluation = LLMFactory.create_llm(fast_config)
            self.llm_correction = LLMFactory.create_llm(fast_config)
            self.llm_search_optimizer = LLMFactory.create_llm(fast_config)
            self.llm_csv_agent = LLMFactory.create_llm(fast_config)
            self.llm_final = LLMFactory.create_llm(fast_config)
            
            logger.info("âœ… TÃ¼m LLM'ler hÄ±zlÄ± OpenAI modeli ile baÅŸlatÄ±ldÄ±")
            
        except Exception as e:
            logger.error(f"âŒ Fast LLM initialization hatasÄ±: {e}")
            raise

    async def _initialize_astradb_native(self):
        """HÄ±zlÄ± AstraDB baÄŸlantÄ±sÄ±"""
        try:
            logger.info("ğŸ”Œ AstraDB native API baÄŸlantÄ±sÄ±...")
            
            astra_client = DataAPIClient(DatabaseSettings.ASTRA_DB_TOKEN)
            self.astra_database = astra_client.get_database(DatabaseSettings.ASTRA_DB_API_ENDPOINT)
            self.astra_collection = self.astra_database.get_collection(DatabaseSettings.ASTRA_DB_COLLECTION)
            
            logger.info(f"âœ… AstraDB baÅŸarÄ±lÄ±")
            
        except Exception as e:
            logger.error(f"âŒ AstraDB hatasÄ±: {e}")
            self.astra_collection = None

    async def _initialize_csv(self):
        """HÄ±zlÄ± CSV yÃ¼kleme"""
        try:
            csv_path = DatabaseSettings.CSV_FILE_PATH
            if csv_path and os.path.exists(csv_path):
                self.csv_data = pd.read_csv(csv_path)
                logger.info(f"âœ… CSV: {len(self.csv_data)} satÄ±r")
            else:
                self.csv_data = None
        except Exception as e:
            logger.warning(f"CSV hatasÄ±: {e}")
            self.csv_data = None

    async def process_message(self, message: str, session_id: str = "default") -> Dict[str, Any]:
        """SÃœPER HIZLI mesaj iÅŸleme - max 20 saniye"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            logger.info(f"âš¡ HIZLI iÅŸleme baÅŸlÄ±yor - Session: {session_id}")
            
            # AdÄ±m 1: HÄ±zlÄ± deÄŸerlendirme (2s max)
            evaluation_task = asyncio.create_task(self._evaluate_question_ultra_fast(message))
            
            # AdÄ±m 2: Paralel dÃ¼zeltme ve context alma (baÅŸla ama bekle)
            correction_task = asyncio.create_task(self._correct_question_fast(message))
            
            # Evaluation sonucunu bekle
            evaluation_result = await evaluation_task
            
            if evaluation_result == "UzmanlÄ±k dÄ±ÅŸÄ± soru":
                return {"response": MessageSettings.ERROR_EXPERTISE_OUT, "sources": []}
            
            if evaluation_result == "SELAMLAMA":
                return {
                    "response": "Merhaba! Size Ã¼niversite tercih konularÄ±nda yardÄ±mcÄ± olabilirim. Hangi konuda bilgi almak istiyorsunuz?",
                    "sources": []
                }
            
            # DÃ¼zeltilmiÅŸ soruyu al
            corrected_question = await correction_task
            
            # AdÄ±m 3: GERÃ‡EK PARALEL context alma (en kritik optimizasyon)
            context_tasks = [
                asyncio.create_task(self._get_vector_context_ultra_fast(corrected_question)),
                asyncio.create_task(self._get_csv_context_ultra_fast(corrected_question))
            ]
            
            # Paralel context'leri bekle (max 10s timeout)
            try:
                context1, context2 = await asyncio.wait_for(
                    asyncio.gather(*context_tasks, return_exceptions=True),
                    timeout=10.0
                )
            except asyncio.TimeoutError:
                logger.warning("âš ï¸ Context timeout, fallback kullanÄ±lÄ±yor")
                context1 = "Vector arama zaman aÅŸÄ±mÄ±"
                context2 = "CSV analizi zaman aÅŸÄ±mÄ±"
            
            # Exception handling
            if isinstance(context1, Exception):
                context1 = "Vector arama baÅŸarÄ±sÄ±z"
            if isinstance(context2, Exception):
                context2 = "CSV analizi baÅŸarÄ±sÄ±z"
            
            # AdÄ±m 4: HÄ±zlÄ± final yanÄ±t
            conversation_history = self.memory.get_history(session_id)
            final_response = await self._generate_final_response_ultra_fast(
                question=corrected_question,
                context1=context1,
                context2=context2,
                history=conversation_history
            )

            # Memory'ye kaydet (async olarak, bekleme)
            asyncio.create_task(self._save_to_memory_async(session_id, message, final_response))

            elapsed = asyncio.get_event_loop().time() - start_time
            logger.info(f"âš¡ Ä°ÅŸlem tamamlandÄ±: {elapsed:.2f}s")

            return {
                "response": final_response,
                "sources": self._extract_sources(context1, context2)
            }
            
        except Exception as e:
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.error(f"âŒ HÄ±zlÄ± iÅŸleme hatasÄ± ({elapsed:.2f}s): {e}")
            return {"response": MessageSettings.ERROR_GENERAL, "sources": []}

    async def _save_to_memory_async(self, session_id: str, user_msg: str, assistant_msg: str):
        """Memory'ye asenkron kaydet - ana iÅŸlemi bloklamaz"""
        try:
            self.memory.add_message(session_id, "user", user_msg)
            self.memory.add_message(session_id, "assistant", assistant_msg)
        except:
            pass  # Memory hatasÄ± ana iÅŸlemi etkilemesin

    async def _evaluate_question_ultra_fast(self, question: str) -> str:
        """Ultra hÄ±zlÄ± deÄŸerlendirme - 2s max"""
        try:
            if not self.llm_evaluation:
                return "UYGUN"
                
            # Timeout ile koruma
            result = await asyncio.wait_for(
                self.llm_evaluation.ainvoke(self.evaluation_prompt.format(question=question)),
                timeout=3.0
            )
            
            evaluation_result = result.content.strip().upper()
            
            if "SELAMLAMA" in evaluation_result:
                return "SELAMLAMA"
            elif "UYGUN" in evaluation_result:
                return "UYGUN"
            else:
                return "UzmanlÄ±k dÄ±ÅŸÄ± soru"
                
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ Evaluation timeout, varsayÄ±lan UYGUN")
            return "UYGUN"
        except Exception as e:
            logger.error(f"âŒ Evaluation hatasÄ±: {e}")
            return "UYGUN"

    async def _correct_question_fast(self, question: str) -> str:
        """HÄ±zlÄ± soru dÃ¼zeltme"""
        try:
            if not self.llm_correction:
                return question
                
            result = await asyncio.wait_for(
                self.llm_correction.ainvoke(self.correction_prompt.format(question=question)),
                timeout=5.0
            )
            
            return result.content.strip()
            
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ Correction timeout, orijinal soru")
            return question
        except Exception as e:
            logger.error(f"âŒ Correction hatasÄ±: {e}")
            return question

    async def _get_vector_context_ultra_fast(self, question: str) -> str:
        """Ultra hÄ±zlÄ± vector search"""
        try:
            if not self.astra_collection:
                return "Vector arama mevcut deÄŸil"
            
            # Query optimization SKIP - direkt ara
            query_embedding = self.get_embedding(question)
            
            # HÄ±zlÄ± arama (limit 2)
            results = self.astra_collection.find(
                {},
                sort={"$vector": query_embedding},
                limit=2  # Daha az sonuÃ§ = daha hÄ±zlÄ±
            )
            
            docs = list(results)
            
            if not docs:
                return "Ä°lgili dokÃ¼man bulunamadÄ±"
            
            # Kompakt context
            context = ""
            for doc in docs:
                file_path = doc.get('metadata', {}).get('file_path', 'Kaynak')
                content = str(doc.get('content', doc.get('text', '')))[:300]  # Daha kÄ±sa
                context += f"{file_path}: {content}\n"
            
            return context
            
        except Exception as e:
            logger.error(f"âŒ Ultra fast vector hatasÄ±: {e}")
            return "Vector arama hatasÄ±"

    async def _get_csv_context_ultra_fast(self, question: str) -> str:
        """Ultra hÄ±zlÄ± CSV analiz"""
        try:
            if self.csv_data is None or not self.llm_csv_agent:
                return "CSV analizi mevcut deÄŸil"
            
            # Basit filtreleme
            question_lower = question.lower()
            
            # Sadece istihdam/maaÅŸ keyword'Ã¼ varsa CSV analizi yap
            csv_keywords = ["istihdam", "maaÅŸ", "gelir", "Ã§alÄ±ÅŸma", "iÅŸ"]
            if not any(kw in question_lower for kw in csv_keywords):
                return "Bu soru iÃ§in CSV analizi gerekli deÄŸil"
            
            # HÄ±zlÄ± sample data
            sample_data = self.csv_data.head(10).to_string(index=False)
            
            result = await asyncio.wait_for(
                self.llm_csv_agent.ainvoke(
                    self.csv_agent_prompt.format(question=question, csv_data=sample_data)
                ),
                timeout=5.0
            )
            
            return result.content.strip()
            
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ CSV timeout")
            return "CSV analizi zaman aÅŸÄ±mÄ±"
        except Exception as e:
            logger.error(f"âŒ Ultra fast CSV hatasÄ±: {e}")
            return "CSV analizi hatasÄ±"

    async def _generate_final_response_ultra_fast(self, question: str, context1: str, context2: str, history: str = "") -> str:
        """Ultra hÄ±zlÄ± final yanÄ±t"""
        try:
            if not self.llm_final:
                return "Sistem geÃ§ici olarak kullanÄ±lamÄ±yor. LÃ¼tfen tekrar deneyin."
            
            result = await asyncio.wait_for(
                self.llm_final.ainvoke(
                    self.final_prompt.format(
                        question=question,
                        context1=context1,
                        context2=context2,
                        history=history[:200]  # History'yi kÄ±salt
                    )
                ),
                timeout=8.0
            )
            
            return result.content.strip()
            
        except asyncio.TimeoutError:
            logger.warning("âš ï¸ Final response timeout")
            return "YanÄ±t oluÅŸturma zaman aÅŸÄ±mÄ±. LÃ¼tfen tekrar deneyin."
        except Exception as e:
            logger.error(f"âŒ Final response hatasÄ±: {e}")
            return "YanÄ±t oluÅŸturulurken hata oluÅŸtu."

    def _extract_sources(self, context1: str, context2: str) -> List[str]:
        """HÄ±zlÄ± kaynak Ã§Ä±karma"""
        sources = []
        
        if context1 and "bulunamadÄ±" not in context1 and "hatasÄ±" not in context1:
            sources.append("Tercih Rehberi DokÃ¼manlarÄ±")
        
        if context2 and "gerekli deÄŸil" not in context2 and "hatasÄ±" not in context2:
            sources.append("CumhurbaÅŸkanlÄ±ÄŸÄ± UNÄ°-VERÄ° veritabanÄ± (2024)")
        
        return sources

    async def test_all_connections(self) -> Dict[str, str]:
        """HÄ±zlÄ± connection test"""
        results = {}
        
        # OpenAI test
        try:
            test_embedding = self.get_embedding("test")
            results["OpenAI"] = f"âœ… BaÄŸlÄ± ({len(test_embedding)} boyut)"
        except Exception as e:
            results["OpenAI"] = f"âŒ Hata: {str(e)[:30]}"
        
        # AstraDB test
        try:
            if self.astra_collection:
                test_results = list(self.astra_collection.find({}, limit=1))
                results["AstraDB"] = f"âœ… BaÄŸlÄ± ({len(test_results)} test)"
            else:
                results["AstraDB"] = "âŒ BaÄŸlantÄ± yok"
        except Exception as e:
            results["AstraDB"] = f"âŒ Hata: {str(e)[:30]}"
        
        # CSV test
        results["CSV"] = f"âœ… {len(self.csv_data)} satÄ±r" if self.csv_data is not None else "âŒ Yok"
        
        return results
